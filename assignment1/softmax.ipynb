{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "  \"\"\"\n",
    "  Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "  it for the linear classifier. These are the same steps as we used for the\n",
    "  SVM, but condensed to a single function.  \n",
    "  \"\"\"\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "  # subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "  mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "  X_dev = X_train[mask]\n",
    "  y_dev = y_train[mask]\n",
    "  \n",
    "  # Preprocessing: reshape the image data into rows\n",
    "  X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "  X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "  X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "  X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "  \n",
    "  # Normalize the data: subtract the mean image\n",
    "  mean_image = np.mean(X_train, axis = 0)\n",
    "  X_train -= mean_image\n",
    "  X_val -= mean_image\n",
    "  X_test -= mean_image\n",
    "  X_dev -= mean_image\n",
    "  \n",
    "  # add bias dimension and transform into columns\n",
    "  X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "  X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "  X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "  X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "  \n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape\n",
    "print 'dev data shape: ', X_dev.shape\n",
    "print 'dev labels shape: ', y_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.388560\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -2.840163 analytic: -2.840163, relative error: 2.456772e-09\n",
      "numerical: 1.242010 analytic: 1.242010, relative error: 4.789745e-08\n",
      "numerical: 1.858591 analytic: 1.858591, relative error: 5.365759e-09\n",
      "numerical: -0.590889 analytic: -0.590889, relative error: 3.558953e-08\n",
      "numerical: -1.969216 analytic: -1.969217, relative error: 3.290069e-08\n",
      "numerical: -1.757410 analytic: -1.757410, relative error: 4.288639e-08\n",
      "numerical: -2.316021 analytic: -2.316021, relative error: 3.511342e-08\n",
      "numerical: 1.857663 analytic: 1.857663, relative error: 2.153946e-08\n",
      "numerical: 3.648791 analytic: 3.648791, relative error: 1.974344e-08\n",
      "numerical: -2.154534 analytic: -2.154534, relative error: 2.396055e-08\n",
      "numerical: -1.285773 analytic: -1.285773, relative error: 1.242462e-08\n",
      "numerical: -1.583017 analytic: -1.583017, relative error: 2.177861e-08\n",
      "numerical: -0.362032 analytic: -0.362032, relative error: 5.954848e-08\n",
      "numerical: 3.001804 analytic: 3.001804, relative error: 1.719944e-08\n",
      "numerical: 0.009670 analytic: 0.009669, relative error: 5.878067e-06\n",
      "numerical: -2.060157 analytic: -2.060157, relative error: 1.259228e-08\n",
      "numerical: 1.754899 analytic: 1.754899, relative error: 1.515548e-08\n",
      "numerical: -0.261636 analytic: -0.261636, relative error: 9.105630e-08\n",
      "numerical: 0.343798 analytic: 0.343798, relative error: 5.192025e-08\n",
      "numerical: 2.934424 analytic: 2.934424, relative error: 1.626377e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.388560e+00 computed in 0.159557s\n",
      "vectorized loss: 2.388560e+00 computed in 0.007464s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 5.756798\n",
      "iteration 100 / 1500: loss 5.515360\n",
      "iteration 200 / 1500: loss 5.831617\n",
      "iteration 300 / 1500: loss 5.981851\n",
      "iteration 400 / 1500: loss 5.380671\n",
      "iteration 500 / 1500: loss 5.685255\n",
      "iteration 600 / 1500: loss 5.477173\n",
      "iteration 700 / 1500: loss 5.428131\n",
      "iteration 800 / 1500: loss 6.013023\n",
      "iteration 900 / 1500: loss 5.834420\n",
      "iteration 1000 / 1500: loss 6.118245\n",
      "iteration 1100 / 1500: loss 5.475968\n",
      "iteration 1200 / 1500: loss 5.657618\n",
      "iteration 1300 / 1500: loss 5.867782\n",
      "iteration 1400 / 1500: loss 5.581142\n",
      "iteration 0 / 1500: loss 6.161524\n",
      "iteration 100 / 1500: loss 6.226817\n",
      "iteration 200 / 1500: loss 6.168880\n",
      "iteration 300 / 1500: loss 5.905097\n",
      "iteration 400 / 1500: loss 5.794676\n",
      "iteration 500 / 1500: loss 5.752672\n",
      "iteration 600 / 1500: loss 6.239279\n",
      "iteration 700 / 1500: loss 7.029293\n",
      "iteration 800 / 1500: loss 5.865288\n",
      "iteration 900 / 1500: loss 5.900658\n",
      "iteration 1000 / 1500: loss 6.127292\n",
      "iteration 1100 / 1500: loss 6.178442\n",
      "iteration 1200 / 1500: loss 6.226556\n",
      "iteration 1300 / 1500: loss 5.918268\n",
      "iteration 1400 / 1500: loss 6.089125\n",
      "iteration 0 / 1500: loss 5.648667\n",
      "iteration 100 / 1500: loss 5.616424\n",
      "iteration 200 / 1500: loss 5.964578\n",
      "iteration 300 / 1500: loss 5.387649\n",
      "iteration 400 / 1500: loss 5.625850\n",
      "iteration 500 / 1500: loss 5.499641\n",
      "iteration 600 / 1500: loss 5.846177\n",
      "iteration 700 / 1500: loss 5.579666\n",
      "iteration 800 / 1500: loss 5.749005\n",
      "iteration 900 / 1500: loss 5.714911\n",
      "iteration 1000 / 1500: loss 5.475528\n",
      "iteration 1100 / 1500: loss 5.912358\n",
      "iteration 1200 / 1500: loss 5.675821\n",
      "iteration 1300 / 1500: loss 5.320587\n",
      "iteration 1400 / 1500: loss 5.399831\n",
      "iteration 0 / 1500: loss 6.001847\n",
      "iteration 100 / 1500: loss 6.543155\n",
      "iteration 200 / 1500: loss 5.842215\n",
      "iteration 300 / 1500: loss 6.328980\n",
      "iteration 400 / 1500: loss 5.856468\n",
      "iteration 500 / 1500: loss 6.534671\n",
      "iteration 600 / 1500: loss 5.914338\n",
      "iteration 700 / 1500: loss 5.882030\n",
      "iteration 800 / 1500: loss 6.482164\n",
      "iteration 900 / 1500: loss 6.665493\n",
      "iteration 1000 / 1500: loss 6.426752\n",
      "iteration 1100 / 1500: loss 6.408653\n",
      "iteration 1200 / 1500: loss 6.464731\n",
      "iteration 1300 / 1500: loss 5.785954\n",
      "iteration 1400 / 1500: loss 6.083627\n",
      "iteration 0 / 1500: loss 5.852595\n",
      "iteration 100 / 1500: loss 5.656404\n",
      "iteration 200 / 1500: loss 5.756369\n",
      "iteration 300 / 1500: loss 6.045899\n",
      "iteration 400 / 1500: loss 5.757657\n",
      "iteration 500 / 1500: loss 5.977360\n",
      "iteration 600 / 1500: loss 5.732406\n",
      "iteration 700 / 1500: loss 5.931233\n",
      "iteration 800 / 1500: loss 5.775546\n",
      "iteration 900 / 1500: loss 6.010192\n",
      "iteration 1000 / 1500: loss 5.530134\n",
      "iteration 1100 / 1500: loss 5.880494\n",
      "iteration 1200 / 1500: loss 5.921283\n",
      "iteration 1300 / 1500: loss 6.064475\n",
      "iteration 1400 / 1500: loss 5.996909\n",
      "iteration 0 / 1500: loss 7.550138\n",
      "iteration 100 / 1500: loss 7.091811\n",
      "iteration 200 / 1500: loss 6.937582\n",
      "iteration 300 / 1500: loss 7.139779\n",
      "iteration 400 / 1500: loss 7.237268\n",
      "iteration 500 / 1500: loss 6.599729\n",
      "iteration 600 / 1500: loss 6.844979\n",
      "iteration 700 / 1500: loss 7.436056\n",
      "iteration 800 / 1500: loss 7.488472\n",
      "iteration 900 / 1500: loss 7.091516\n",
      "iteration 1000 / 1500: loss 7.345163\n",
      "iteration 1100 / 1500: loss 6.987614\n",
      "iteration 1200 / 1500: loss 6.960619\n",
      "iteration 1300 / 1500: loss 6.877075\n",
      "iteration 1400 / 1500: loss 7.588370\n",
      "iteration 0 / 1500: loss 20.186738\n",
      "iteration 100 / 1500: loss 20.155194\n",
      "iteration 200 / 1500: loss 20.025563\n",
      "iteration 300 / 1500: loss 19.981571\n",
      "iteration 400 / 1500: loss 20.081725\n",
      "iteration 500 / 1500: loss 20.142117\n",
      "iteration 600 / 1500: loss 20.037188\n",
      "iteration 700 / 1500: loss 20.099252\n",
      "iteration 800 / 1500: loss 19.834937\n",
      "iteration 900 / 1500: loss 19.678753\n",
      "iteration 1000 / 1500: loss 19.987690\n",
      "iteration 1100 / 1500: loss 19.423272\n",
      "iteration 1200 / 1500: loss 19.879314\n",
      "iteration 1300 / 1500: loss 20.195290\n",
      "iteration 1400 / 1500: loss 19.890961\n",
      "iteration 0 / 1500: loss 158.039153\n",
      "iteration 100 / 1500: loss 157.765700\n",
      "iteration 200 / 1500: loss 157.992447\n",
      "iteration 300 / 1500: loss 157.470613\n",
      "iteration 400 / 1500: loss 157.783456\n",
      "iteration 500 / 1500: loss 157.941550\n",
      "iteration 600 / 1500: loss 157.791239\n",
      "iteration 700 / 1500: loss 157.948552\n",
      "iteration 800 / 1500: loss 157.801250\n",
      "iteration 900 / 1500: loss 157.390409\n",
      "iteration 1000 / 1500: loss 157.824453\n",
      "iteration 1100 / 1500: loss 157.226539\n",
      "iteration 1200 / 1500: loss 157.405451\n",
      "iteration 1300 / 1500: loss 157.747576\n",
      "iteration 1400 / 1500: loss 157.362150\n",
      "iteration 0 / 1500: loss 1549.910630\n",
      "iteration 100 / 1500: loss 1546.989241\n",
      "iteration 200 / 1500: loss 1543.982065\n",
      "iteration 300 / 1500: loss 1540.659806\n",
      "iteration 400 / 1500: loss 1537.781965\n",
      "iteration 500 / 1500: loss 1534.310268\n",
      "iteration 600 / 1500: loss 1531.453481\n",
      "iteration 700 / 1500: loss 1528.997623\n",
      "iteration 800 / 1500: loss 1525.492702\n",
      "iteration 900 / 1500: loss 1522.328025\n",
      "iteration 1000 / 1500: loss 1519.112589\n",
      "iteration 1100 / 1500: loss 1516.247607\n",
      "iteration 1200 / 1500: loss 1512.945678\n",
      "iteration 1300 / 1500: loss 1510.410560\n",
      "iteration 1400 / 1500: loss 1506.884444\n",
      "iteration 0 / 1500: loss 15503.136317\n",
      "iteration 100 / 1500: loss 15196.198688\n",
      "iteration 200 / 1500: loss 14895.169244\n",
      "iteration 300 / 1500: loss 14600.477819\n",
      "iteration 400 / 1500: loss 14311.406210\n",
      "iteration 500 / 1500: loss 14027.427798\n",
      "iteration 600 / 1500: loss 13749.837308\n",
      "iteration 700 / 1500: loss 13477.239342\n",
      "iteration 800 / 1500: loss 13211.345495\n",
      "iteration 900 / 1500: loss 12949.635082\n",
      "iteration 1000 / 1500: loss 12692.673750\n",
      "iteration 1100 / 1500: loss 12442.092719\n",
      "iteration 1200 / 1500: loss 12195.043579\n",
      "iteration 1300 / 1500: loss 11953.428642\n",
      "iteration 1400 / 1500: loss 11716.992225\n",
      "iteration 0 / 1500: loss 6.021867\n",
      "iteration 100 / 1500: loss 5.725789\n",
      "iteration 200 / 1500: loss 4.921471\n",
      "iteration 300 / 1500: loss 4.356617\n",
      "iteration 400 / 1500: loss 4.542322\n",
      "iteration 500 / 1500: loss 4.519049\n",
      "iteration 600 / 1500: loss 4.327947\n",
      "iteration 700 / 1500: loss 4.097391\n",
      "iteration 800 / 1500: loss 3.878258\n",
      "iteration 900 / 1500: loss 3.963614\n",
      "iteration 1000 / 1500: loss 3.756668\n",
      "iteration 1100 / 1500: loss 3.701541\n",
      "iteration 1200 / 1500: loss 3.637214\n",
      "iteration 1300 / 1500: loss 3.432080\n",
      "iteration 1400 / 1500: loss 3.505377\n",
      "iteration 0 / 1500: loss 5.678899\n",
      "iteration 100 / 1500: loss 5.219017\n",
      "iteration 200 / 1500: loss 5.086069\n",
      "iteration 300 / 1500: loss 4.904062\n",
      "iteration 400 / 1500: loss 4.292672\n",
      "iteration 500 / 1500: loss 4.781607\n",
      "iteration 600 / 1500: loss 4.257994\n",
      "iteration 700 / 1500: loss 3.977967\n",
      "iteration 800 / 1500: loss 4.214259\n",
      "iteration 900 / 1500: loss 4.075847\n",
      "iteration 1000 / 1500: loss 3.561704\n",
      "iteration 1100 / 1500: loss 3.781623\n",
      "iteration 1200 / 1500: loss 4.187049\n",
      "iteration 1300 / 1500: loss 3.738155\n",
      "iteration 1400 / 1500: loss 3.708004\n",
      "iteration 0 / 1500: loss 5.713817\n",
      "iteration 100 / 1500: loss 5.243393\n",
      "iteration 200 / 1500: loss 5.196080\n",
      "iteration 300 / 1500: loss 4.568151\n",
      "iteration 400 / 1500: loss 4.126819\n",
      "iteration 500 / 1500: loss 3.812745\n",
      "iteration 600 / 1500: loss 3.852809\n",
      "iteration 700 / 1500: loss 3.722919\n",
      "iteration 800 / 1500: loss 4.098838\n",
      "iteration 900 / 1500: loss 3.727718\n",
      "iteration 1000 / 1500: loss 3.458465\n",
      "iteration 1100 / 1500: loss 3.567421\n",
      "iteration 1200 / 1500: loss 3.590812\n",
      "iteration 1300 / 1500: loss 3.590082\n",
      "iteration 1400 / 1500: loss 3.364825\n",
      "iteration 0 / 1500: loss 6.214072\n",
      "iteration 100 / 1500: loss 5.364866\n",
      "iteration 200 / 1500: loss 4.521519\n",
      "iteration 300 / 1500: loss 4.625035\n",
      "iteration 400 / 1500: loss 3.950175\n",
      "iteration 500 / 1500: loss 4.044534\n",
      "iteration 600 / 1500: loss 3.904843\n",
      "iteration 700 / 1500: loss 4.056921\n",
      "iteration 800 / 1500: loss 3.791082\n",
      "iteration 900 / 1500: loss 4.010950\n",
      "iteration 1000 / 1500: loss 3.748685\n",
      "iteration 1100 / 1500: loss 3.535327\n",
      "iteration 1200 / 1500: loss 3.421647\n",
      "iteration 1300 / 1500: loss 3.837235\n",
      "iteration 1400 / 1500: loss 3.687720\n",
      "iteration 0 / 1500: loss 6.108034\n",
      "iteration 100 / 1500: loss 5.710890\n",
      "iteration 200 / 1500: loss 5.012963\n",
      "iteration 300 / 1500: loss 4.966834\n",
      "iteration 400 / 1500: loss 4.947746\n",
      "iteration 500 / 1500: loss 4.245463\n",
      "iteration 600 / 1500: loss 4.432691\n",
      "iteration 700 / 1500: loss 4.507674\n",
      "iteration 800 / 1500: loss 4.325945\n",
      "iteration 900 / 1500: loss 4.399359\n",
      "iteration 1000 / 1500: loss 3.863302\n",
      "iteration 1100 / 1500: loss 4.030687\n",
      "iteration 1200 / 1500: loss 3.789273\n",
      "iteration 1300 / 1500: loss 3.512379\n",
      "iteration 1400 / 1500: loss 3.630939\n",
      "iteration 0 / 1500: loss 6.435254\n",
      "iteration 100 / 1500: loss 6.152414\n",
      "iteration 200 / 1500: loss 5.935977\n",
      "iteration 300 / 1500: loss 5.751950\n",
      "iteration 400 / 1500: loss 5.577898\n",
      "iteration 500 / 1500: loss 5.498854\n",
      "iteration 600 / 1500: loss 5.168036\n",
      "iteration 700 / 1500: loss 5.849010\n",
      "iteration 800 / 1500: loss 5.561580\n",
      "iteration 900 / 1500: loss 5.193917\n",
      "iteration 1000 / 1500: loss 5.521307\n",
      "iteration 1100 / 1500: loss 4.993665\n",
      "iteration 1200 / 1500: loss 4.862995\n",
      "iteration 1300 / 1500: loss 5.245944\n",
      "iteration 1400 / 1500: loss 5.235034\n",
      "iteration 0 / 1500: loss 22.038428\n",
      "iteration 100 / 1500: loss 21.469945\n",
      "iteration 200 / 1500: loss 20.391415\n",
      "iteration 300 / 1500: loss 19.921657\n",
      "iteration 400 / 1500: loss 19.809876\n",
      "iteration 500 / 1500: loss 19.585796\n",
      "iteration 600 / 1500: loss 19.213984\n",
      "iteration 700 / 1500: loss 19.096419\n",
      "iteration 800 / 1500: loss 18.723936\n",
      "iteration 900 / 1500: loss 18.882057\n",
      "iteration 1000 / 1500: loss 18.368682\n",
      "iteration 1100 / 1500: loss 18.720932\n",
      "iteration 1200 / 1500: loss 18.423541\n",
      "iteration 1300 / 1500: loss 18.176605\n",
      "iteration 1400 / 1500: loss 18.214392\n",
      "iteration 0 / 1500: loss 158.530030\n",
      "iteration 100 / 1500: loss 153.101164\n",
      "iteration 200 / 1500: loss 147.811561\n",
      "iteration 300 / 1500: loss 142.763243\n",
      "iteration 400 / 1500: loss 138.119604\n",
      "iteration 500 / 1500: loss 133.366617\n",
      "iteration 600 / 1500: loss 128.760410\n",
      "iteration 700 / 1500: loss 124.747327\n",
      "iteration 800 / 1500: loss 120.458077\n",
      "iteration 900 / 1500: loss 116.528540\n",
      "iteration 1000 / 1500: loss 112.749970\n",
      "iteration 1100 / 1500: loss 108.919132\n",
      "iteration 1200 / 1500: loss 105.438369\n",
      "iteration 1300 / 1500: loss 101.743500\n",
      "iteration 1400 / 1500: loss 98.521033\n",
      "iteration 0 / 1500: loss 1535.252833\n",
      "iteration 100 / 1500: loss 1099.410832\n",
      "iteration 200 / 1500: loss 787.171249\n",
      "iteration 300 / 1500: loss 564.241862\n",
      "iteration 400 / 1500: loss 404.687581\n",
      "iteration 500 / 1500: loss 290.071850\n",
      "iteration 600 / 1500: loss 208.168618\n",
      "iteration 700 / 1500: loss 149.535111\n",
      "iteration 800 / 1500: loss 107.806347\n",
      "iteration 900 / 1500: loss 77.751846\n",
      "iteration 1000 / 1500: loss 56.293747\n",
      "iteration 1100 / 1500: loss 40.886963\n",
      "iteration 1200 / 1500: loss 29.835438\n",
      "iteration 1300 / 1500: loss 21.929626\n",
      "iteration 1400 / 1500: loss 16.286507\n",
      "iteration 0 / 1500: loss 15146.831868\n",
      "iteration 100 / 1500: loss 525.680099\n",
      "iteration 200 / 1500: loss 20.358251\n",
      "iteration 300 / 1500: loss 2.906743\n",
      "iteration 400 / 1500: loss 2.298546\n",
      "iteration 500 / 1500: loss 2.272768\n",
      "iteration 600 / 1500: loss 2.261980\n",
      "iteration 700 / 1500: loss 2.262042\n",
      "iteration 800 / 1500: loss 2.260891\n",
      "iteration 900 / 1500: loss 2.268745\n",
      "iteration 1000 / 1500: loss 2.266106\n",
      "iteration 1100 / 1500: loss 2.261637\n",
      "iteration 1200 / 1500: loss 2.259243\n",
      "iteration 1300 / 1500: loss 2.258831\n",
      "iteration 1400 / 1500: loss 2.276041\n",
      "iteration 0 / 1500: loss 6.176965\n",
      "iteration 100 / 1500: loss 2.240642\n",
      "iteration 200 / 1500: loss 2.385322\n",
      "iteration 300 / 1500: loss 2.148096\n",
      "iteration 400 / 1500: loss 2.065721\n",
      "iteration 500 / 1500: loss 1.842890\n",
      "iteration 600 / 1500: loss 1.994119\n",
      "iteration 700 / 1500: loss 1.853271\n",
      "iteration 800 / 1500: loss 1.807610\n",
      "iteration 900 / 1500: loss 2.127933\n",
      "iteration 1000 / 1500: loss 1.815591\n",
      "iteration 1100 / 1500: loss 1.977520\n",
      "iteration 1200 / 1500: loss 1.815208\n",
      "iteration 1300 / 1500: loss 1.845475\n",
      "iteration 1400 / 1500: loss 1.750592\n",
      "iteration 0 / 1500: loss 5.156883\n",
      "iteration 100 / 1500: loss 2.338274\n",
      "iteration 200 / 1500: loss 2.153168\n",
      "iteration 300 / 1500: loss 2.018309\n",
      "iteration 400 / 1500: loss 1.939826\n",
      "iteration 500 / 1500: loss 2.012526\n",
      "iteration 600 / 1500: loss 2.039056\n",
      "iteration 700 / 1500: loss 1.869320\n",
      "iteration 800 / 1500: loss 1.937704\n",
      "iteration 900 / 1500: loss 1.908898\n",
      "iteration 1000 / 1500: loss 1.686292\n",
      "iteration 1100 / 1500: loss 1.832952\n",
      "iteration 1200 / 1500: loss 1.918986\n",
      "iteration 1300 / 1500: loss 1.999832\n",
      "iteration 1400 / 1500: loss 1.706107\n",
      "iteration 0 / 1500: loss 4.942200\n",
      "iteration 100 / 1500: loss 2.523390\n",
      "iteration 200 / 1500: loss 2.270748\n",
      "iteration 300 / 1500: loss 2.228426\n",
      "iteration 400 / 1500: loss 1.842437\n",
      "iteration 500 / 1500: loss 1.864348\n",
      "iteration 600 / 1500: loss 1.897375\n",
      "iteration 700 / 1500: loss 1.868099\n",
      "iteration 800 / 1500: loss 1.825970\n",
      "iteration 900 / 1500: loss 2.013967\n",
      "iteration 1000 / 1500: loss 1.919627\n",
      "iteration 1100 / 1500: loss 1.738044\n",
      "iteration 1200 / 1500: loss 2.010227\n",
      "iteration 1300 / 1500: loss 1.953245\n",
      "iteration 1400 / 1500: loss 1.786812\n",
      "iteration 0 / 1500: loss 6.162072\n",
      "iteration 100 / 1500: loss 2.527344\n",
      "iteration 200 / 1500: loss 2.090514\n",
      "iteration 300 / 1500: loss 2.241172\n",
      "iteration 400 / 1500: loss 1.994868\n",
      "iteration 500 / 1500: loss 2.055142\n",
      "iteration 600 / 1500: loss 2.033547\n",
      "iteration 700 / 1500: loss 2.030433\n",
      "iteration 800 / 1500: loss 1.935759\n",
      "iteration 900 / 1500: loss 1.879049\n",
      "iteration 1000 / 1500: loss 1.829328\n",
      "iteration 1100 / 1500: loss 1.921802\n",
      "iteration 1200 / 1500: loss 1.837591\n",
      "iteration 1300 / 1500: loss 1.684531\n",
      "iteration 1400 / 1500: loss 1.704151\n",
      "iteration 0 / 1500: loss 5.998912\n",
      "iteration 100 / 1500: loss 2.383985\n",
      "iteration 200 / 1500: loss 2.465925\n",
      "iteration 300 / 1500: loss 2.204170\n",
      "iteration 400 / 1500: loss 2.181108\n",
      "iteration 500 / 1500: loss 2.172622\n",
      "iteration 600 / 1500: loss 2.194766\n",
      "iteration 700 / 1500: loss 2.119134\n",
      "iteration 800 / 1500: loss 1.933410\n",
      "iteration 900 / 1500: loss 1.987561\n",
      "iteration 1000 / 1500: loss 2.046221\n",
      "iteration 1100 / 1500: loss 1.977894\n",
      "iteration 1200 / 1500: loss 2.116558\n",
      "iteration 1300 / 1500: loss 1.892742\n",
      "iteration 1400 / 1500: loss 1.929933\n",
      "iteration 0 / 1500: loss 7.462916\n",
      "iteration 100 / 1500: loss 3.924060\n",
      "iteration 200 / 1500: loss 3.629972\n",
      "iteration 300 / 1500: loss 3.357655\n",
      "iteration 400 / 1500: loss 3.120407\n",
      "iteration 500 / 1500: loss 3.088214\n",
      "iteration 600 / 1500: loss 2.947568\n",
      "iteration 700 / 1500: loss 2.717043\n",
      "iteration 800 / 1500: loss 2.880231\n",
      "iteration 900 / 1500: loss 2.764850\n",
      "iteration 1000 / 1500: loss 2.657239\n",
      "iteration 1100 / 1500: loss 2.513524\n",
      "iteration 1200 / 1500: loss 2.520549\n",
      "iteration 1300 / 1500: loss 2.535273\n",
      "iteration 1400 / 1500: loss 2.405575\n",
      "iteration 0 / 1500: loss 20.876680\n",
      "iteration 100 / 1500: loss 10.422809\n",
      "iteration 200 / 1500: loss 6.737022\n",
      "iteration 300 / 1500: loss 4.525837\n",
      "iteration 400 / 1500: loss 3.359130\n",
      "iteration 500 / 1500: loss 2.732837\n",
      "iteration 600 / 1500: loss 2.243316\n",
      "iteration 700 / 1500: loss 2.016529\n",
      "iteration 800 / 1500: loss 2.046495\n",
      "iteration 900 / 1500: loss 1.926028\n",
      "iteration 1000 / 1500: loss 1.839105\n",
      "iteration 1100 / 1500: loss 1.900770\n",
      "iteration 1200 / 1500: loss 1.714422\n",
      "iteration 1300 / 1500: loss 1.845282\n",
      "iteration 1400 / 1500: loss 1.824672\n",
      "iteration 0 / 1500: loss 158.162674\n",
      "iteration 100 / 1500: loss 2.479981\n",
      "iteration 200 / 1500: loss 1.981669\n",
      "iteration 300 / 1500: loss 1.997796\n",
      "iteration 400 / 1500: loss 1.958634\n",
      "iteration 500 / 1500: loss 1.921218\n",
      "iteration 600 / 1500: loss 2.069826\n",
      "iteration 700 / 1500: loss 1.985217\n",
      "iteration 800 / 1500: loss 1.971939\n",
      "iteration 900 / 1500: loss 1.979271\n",
      "iteration 1000 / 1500: loss 1.920191\n",
      "iteration 1100 / 1500: loss 1.999542\n",
      "iteration 1200 / 1500: loss 1.956041\n",
      "iteration 1300 / 1500: loss 1.911687\n",
      "iteration 1400 / 1500: loss 2.065436\n",
      "iteration 0 / 1500: loss 1565.281312\n",
      "iteration 100 / 1500: loss 2.275081\n",
      "iteration 200 / 1500: loss 2.192057\n",
      "iteration 300 / 1500: loss 2.164104\n",
      "iteration 400 / 1500: loss 2.217498\n",
      "iteration 500 / 1500: loss 2.184937\n",
      "iteration 600 / 1500: loss 2.191988\n",
      "iteration 700 / 1500: loss 2.210085\n",
      "iteration 800 / 1500: loss 2.156013\n",
      "iteration 900 / 1500: loss 2.182079\n",
      "iteration 1000 / 1500: loss 2.257041\n",
      "iteration 1100 / 1500: loss 2.207426\n",
      "iteration 1200 / 1500: loss 2.224896\n",
      "iteration 1300 / 1500: loss 2.225697\n",
      "iteration 1400 / 1500: loss 2.214369\n",
      "iteration 0 / 1500: loss 15273.242292\n",
      "iteration 100 / 1500: loss 2472226908209620825923201181996054028394520698041663488.000000\n",
      "iteration 200 / 1500: loss 399843994587488952650328570623511611333836179712435955244810505839668927874916848277042827492608744357888.000000\n",
      "iteration 300 / 1500: loss 64668505741433343435564444176354582082859795148863427878903169275856512689261158424067809244085725456843304903946256456421610421291359345956703768368119808.000000\n",
      "iteration 400 / 1500: loss 10459118284730775686480698530735089599527231722826570960929264700957307612876254082165692254131187797679671875179593552432419355639985173087074167328702126534939899060306153745661455285897486754687538429952.000000\n",
      "iteration 500 / 1500: loss 1691598623468750353138524757838630093762995317734039681893383562452372953385778994906837268401457371590346915838669130239011298009258953188022972221010669364557362772427713246697409578792804584707122871170786680613881606745116712850367756950918982252101632.000000\n",
      "iteration 600 / 1500: loss 273589591877823118028883804293456482901542167662230125781260279557754201244433752171630077848573436686812160517430725694779077209498284463398066430777779868219347551299980870869640685223976857968553144617109018056682630243771295038474298239754797603261216032498007052243995798195224800411461094114494251008.000000\n",
      "iteration 700 / 1500: loss inf\n",
      "iteration 800 / 1500: loss inf\n",
      "iteration 900 / 1500: loss inf\n",
      "iteration 1000 / 1500: loss inf\n",
      "iteration 1100 / 1500: loss inf\n",
      "iteration 1200 / 1500: loss inf\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.810743\n",
      "iteration 100 / 1500: loss 187.154094\n",
      "iteration 200 / 1500: loss 158.091335\n",
      "iteration 300 / 1500: loss 114.305520\n",
      "iteration 400 / 1500: loss 121.960361\n",
      "iteration 500 / 1500: loss 132.311955\n",
      "iteration 600 / 1500: loss 129.612084\n",
      "iteration 700 / 1500: loss 160.169241\n",
      "iteration 800 / 1500: loss 114.056148\n",
      "iteration 900 / 1500: loss 118.061189\n",
      "iteration 1000 / 1500: loss 139.952825\n",
      "iteration 1100 / 1500: loss 97.504020\n",
      "iteration 1200 / 1500: loss 107.839960\n",
      "iteration 1300 / 1500: loss 125.131952\n",
      "iteration 1400 / 1500: loss 100.328835\n",
      "iteration 0 / 1500: loss 5.440726\n",
      "iteration 100 / 1500: loss 92.628721\n",
      "iteration 200 / 1500: loss 100.791956\n",
      "iteration 300 / 1500: loss 121.303673\n",
      "iteration 400 / 1500: loss 110.898858\n",
      "iteration 500 / 1500: loss 87.425320\n",
      "iteration 600 / 1500: loss 115.995149\n",
      "iteration 700 / 1500: loss 131.784752\n",
      "iteration 800 / 1500: loss 165.584038\n",
      "iteration 900 / 1500: loss 100.897982\n",
      "iteration 1000 / 1500: loss 119.444214\n",
      "iteration 1100 / 1500: loss 114.848865\n",
      "iteration 1200 / 1500: loss 94.762249\n",
      "iteration 1300 / 1500: loss 187.155873\n",
      "iteration 1400 / 1500: loss 124.810232\n",
      "iteration 0 / 1500: loss 5.179376\n",
      "iteration 100 / 1500: loss 142.024520\n",
      "iteration 200 / 1500: loss 109.059342\n",
      "iteration 300 / 1500: loss 192.727293\n",
      "iteration 400 / 1500: loss 133.715126\n",
      "iteration 500 / 1500: loss 105.337858\n",
      "iteration 600 / 1500: loss 142.652548\n",
      "iteration 700 / 1500: loss 169.579146\n",
      "iteration 800 / 1500: loss 249.581919\n",
      "iteration 900 / 1500: loss 138.169036\n",
      "iteration 1000 / 1500: loss 109.697733\n",
      "iteration 1100 / 1500: loss 156.778922\n",
      "iteration 1200 / 1500: loss 119.505358\n",
      "iteration 1300 / 1500: loss 126.293749\n",
      "iteration 1400 / 1500: loss 148.064130\n",
      "iteration 0 / 1500: loss 6.807558\n",
      "iteration 100 / 1500: loss 147.248069\n",
      "iteration 200 / 1500: loss 128.193497\n",
      "iteration 300 / 1500: loss 126.563036\n",
      "iteration 400 / 1500: loss 115.700908\n",
      "iteration 500 / 1500: loss 145.197522\n",
      "iteration 600 / 1500: loss 156.785339\n",
      "iteration 700 / 1500: loss 90.696350\n",
      "iteration 800 / 1500: loss 147.853864\n",
      "iteration 900 / 1500: loss 127.914610\n",
      "iteration 1000 / 1500: loss 104.773626\n",
      "iteration 1100 / 1500: loss 124.834167\n",
      "iteration 1200 / 1500: loss 136.798934\n",
      "iteration 1300 / 1500: loss 147.483200\n",
      "iteration 1400 / 1500: loss 106.932738\n",
      "iteration 0 / 1500: loss 5.472959\n",
      "iteration 100 / 1500: loss 109.910233\n",
      "iteration 200 / 1500: loss 129.544297\n",
      "iteration 300 / 1500: loss 107.099275\n",
      "iteration 400 / 1500: loss 124.493883\n",
      "iteration 500 / 1500: loss 119.539544\n",
      "iteration 600 / 1500: loss 108.385949\n",
      "iteration 700 / 1500: loss 147.191528\n",
      "iteration 800 / 1500: loss 100.557071\n",
      "iteration 900 / 1500: loss 141.583627\n",
      "iteration 1000 / 1500: loss 110.037400\n",
      "iteration 1100 / 1500: loss 204.734447\n",
      "iteration 1200 / 1500: loss 189.490535\n",
      "iteration 1300 / 1500: loss 134.606754\n",
      "iteration 1400 / 1500: loss 128.660465\n",
      "iteration 0 / 1500: loss 8.388760\n",
      "iteration 100 / 1500: loss 224.951225\n",
      "iteration 200 / 1500: loss 174.895468\n",
      "iteration 300 / 1500: loss 187.268249\n",
      "iteration 400 / 1500: loss 193.367880\n",
      "iteration 500 / 1500: loss 239.428897\n",
      "iteration 600 / 1500: loss 234.014335\n",
      "iteration 700 / 1500: loss 188.588179\n",
      "iteration 800 / 1500: loss 189.248735\n",
      "iteration 900 / 1500: loss 171.231109\n",
      "iteration 1000 / 1500: loss 191.647281\n",
      "iteration 1100 / 1500: loss 108.750783\n",
      "iteration 1200 / 1500: loss 195.455445\n",
      "iteration 1300 / 1500: loss 290.370899\n",
      "iteration 1400 / 1500: loss 149.805281\n",
      "iteration 0 / 1500: loss 20.880368\n",
      "iteration 100 / 1500: loss 335.282041\n",
      "iteration 200 / 1500: loss 290.795672\n",
      "iteration 300 / 1500: loss 456.074959\n",
      "iteration 400 / 1500: loss 390.860456\n",
      "iteration 500 / 1500: loss 333.039130\n",
      "iteration 600 / 1500: loss 326.706076\n",
      "iteration 700 / 1500: loss 339.392649\n",
      "iteration 800 / 1500: loss 324.426437\n",
      "iteration 900 / 1500: loss 497.427692\n",
      "iteration 1000 / 1500: loss 386.440308\n",
      "iteration 1100 / 1500: loss 345.549518\n",
      "iteration 1200 / 1500: loss 276.395221\n",
      "iteration 1300 / 1500: loss 253.985109\n",
      "iteration 1400 / 1500: loss 338.107283\n",
      "iteration 0 / 1500: loss 160.828859\n",
      "iteration 100 / 1500: loss 4652804584133435920606947629203419011864585256581676574404955604836886609844642973207163792562654639371689821995008.000000\n",
      "iteration 200 / 1500: loss 84313389559265992145130321063199407061356743026679420955543582004318684821277190697412104086875440132239557483830307849181097605534155798306900081415569622105916191615030060210342800320538608587814639235162932977244613321424896.000000\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss inf\n",
      "iteration 500 / 1500: loss inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/softmax.py:84: RuntimeWarning: overflow encountered in subtract\n",
      "  scores -= np.c_[np.max(scores, axis = 1)]\n",
      "cs231n/classifiers/softmax.py:84: RuntimeWarning: invalid value encountered in subtract\n",
      "  scores -= np.c_[np.max(scores, axis = 1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1539.018694\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15396.397182\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.987209\n",
      "iteration 100 / 1500: loss 17562.401260\n",
      "iteration 200 / 1500: loss 24853.241424\n",
      "iteration 300 / 1500: loss 29669.216316\n",
      "iteration 400 / 1500: loss 14258.341802\n",
      "iteration 500 / 1500: loss 30928.397464\n",
      "iteration 600 / 1500: loss 25629.081086\n",
      "iteration 700 / 1500: loss 19180.375079\n",
      "iteration 800 / 1500: loss 30350.970551\n",
      "iteration 900 / 1500: loss 29225.888936\n",
      "iteration 1000 / 1500: loss 27139.998880\n",
      "iteration 1100 / 1500: loss 25009.887944\n",
      "iteration 1200 / 1500: loss 19513.715018\n",
      "iteration 1300 / 1500: loss 16021.857199\n",
      "iteration 1400 / 1500: loss 25834.525645\n",
      "iteration 0 / 1500: loss 5.386320\n",
      "iteration 100 / 1500: loss 35361.843237\n",
      "iteration 200 / 1500: loss 29675.354559\n",
      "iteration 300 / 1500: loss 21199.251276\n",
      "iteration 400 / 1500: loss 26890.507875\n",
      "iteration 500 / 1500: loss 25413.376967\n",
      "iteration 600 / 1500: loss 19333.916494\n",
      "iteration 700 / 1500: loss 16342.564531\n",
      "iteration 800 / 1500: loss 22661.491428\n",
      "iteration 900 / 1500: loss 19109.748652\n",
      "iteration 1000 / 1500: loss 17572.489205\n",
      "iteration 1100 / 1500: loss 16846.393598\n",
      "iteration 1200 / 1500: loss 26434.549307\n",
      "iteration 1300 / 1500: loss 25554.020680\n",
      "iteration 1400 / 1500: loss 21363.747762\n",
      "iteration 0 / 1500: loss 5.631538\n",
      "iteration 100 / 1500: loss 29338.010416\n",
      "iteration 200 / 1500: loss 13901.674688\n",
      "iteration 300 / 1500: loss 26991.650623\n",
      "iteration 400 / 1500: loss 33409.480493\n",
      "iteration 500 / 1500: loss 35560.519790\n",
      "iteration 600 / 1500: loss 19907.975806\n",
      "iteration 700 / 1500: loss 20673.726740\n",
      "iteration 800 / 1500: loss 22536.393626\n",
      "iteration 900 / 1500: loss 30677.898328\n",
      "iteration 1000 / 1500: loss 32286.687748\n",
      "iteration 1100 / 1500: loss 24416.019919\n",
      "iteration 1200 / 1500: loss 26048.849693\n",
      "iteration 1300 / 1500: loss 40667.228496\n",
      "iteration 1400 / 1500: loss 36775.682603\n",
      "iteration 0 / 1500: loss 5.948589\n",
      "iteration 100 / 1500: loss 48578.828800\n",
      "iteration 200 / 1500: loss 43921.354785\n",
      "iteration 300 / 1500: loss 42870.169296\n",
      "iteration 400 / 1500: loss 23012.774148\n",
      "iteration 500 / 1500: loss 38404.574594\n",
      "iteration 600 / 1500: loss 29612.426747\n",
      "iteration 700 / 1500: loss 33026.348633\n",
      "iteration 800 / 1500: loss 19890.245344\n",
      "iteration 900 / 1500: loss 26933.158837\n",
      "iteration 1000 / 1500: loss 23778.410565\n",
      "iteration 1100 / 1500: loss 46868.517944\n",
      "iteration 1200 / 1500: loss 33138.149255\n",
      "iteration 1300 / 1500: loss 38277.769679\n",
      "iteration 1400 / 1500: loss 36311.826186\n",
      "iteration 0 / 1500: loss 6.395244\n",
      "iteration 100 / 1500: loss 86747.695046\n",
      "iteration 200 / 1500: loss 86403.265721\n",
      "iteration 300 / 1500: loss 97316.852680\n",
      "iteration 400 / 1500: loss 82810.488463\n",
      "iteration 500 / 1500: loss 82847.680900\n",
      "iteration 600 / 1500: loss 99540.156215\n",
      "iteration 700 / 1500: loss 107913.434088\n",
      "iteration 800 / 1500: loss 91964.183745\n",
      "iteration 900 / 1500: loss 81405.793353\n",
      "iteration 1000 / 1500: loss 101669.975521\n",
      "iteration 1100 / 1500: loss 94147.445629\n",
      "iteration 1200 / 1500: loss 98050.063301\n",
      "iteration 1300 / 1500: loss 122872.616120\n",
      "iteration 1400 / 1500: loss 78813.823365\n",
      "iteration 0 / 1500: loss 7.165335\n",
      "iteration 100 / 1500: loss 20267218824941418612092788509921715896469347810217590548996710753288000817308127163717934600783098565415647062212110498060460616170681674971186087063129415334331938766848.000000\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss inf\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 21.811822\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 160.597540\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1572.334384\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15439.283024\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.854640\n",
      "iteration 100 / 1500: loss 3426655.210873\n",
      "iteration 200 / 1500: loss 7303794.054006\n",
      "iteration 300 / 1500: loss 4012200.570730\n",
      "iteration 400 / 1500: loss 2643195.591116\n",
      "iteration 500 / 1500: loss 3554350.875419\n",
      "iteration 600 / 1500: loss 3137769.878380\n",
      "iteration 700 / 1500: loss 4475088.163108\n",
      "iteration 800 / 1500: loss 5436627.539650\n",
      "iteration 900 / 1500: loss 2449506.929094\n",
      "iteration 1000 / 1500: loss 5327153.246274\n",
      "iteration 1100 / 1500: loss 6935911.534829\n",
      "iteration 1200 / 1500: loss 3206712.339703\n",
      "iteration 1300 / 1500: loss 5484862.223171\n",
      "iteration 1400 / 1500: loss 4380595.282859\n",
      "iteration 0 / 1500: loss 4.765780\n",
      "iteration 100 / 1500: loss 6096784.306922\n",
      "iteration 200 / 1500: loss 5913733.209479\n",
      "iteration 300 / 1500: loss 6434786.149543\n",
      "iteration 400 / 1500: loss 4374856.466760\n",
      "iteration 500 / 1500: loss 5844299.581291\n",
      "iteration 600 / 1500: loss 7096706.070967\n",
      "iteration 700 / 1500: loss 6260179.670027\n",
      "iteration 800 / 1500: loss 8193530.005500\n",
      "iteration 900 / 1500: loss 5913405.987437\n",
      "iteration 1000 / 1500: loss 6935379.435907\n",
      "iteration 1100 / 1500: loss 6495465.630082\n",
      "iteration 1200 / 1500: loss 5024030.497951\n",
      "iteration 1300 / 1500: loss 7280337.793264\n",
      "iteration 1400 / 1500: loss 4136413.941621\n",
      "iteration 0 / 1500: loss 6.046754\n",
      "iteration 100 / 1500: loss 44393175.061187\n",
      "iteration 200 / 1500: loss 42377291.166128\n",
      "iteration 300 / 1500: loss 42719840.762539\n",
      "iteration 400 / 1500: loss 38462882.140715\n",
      "iteration 500 / 1500: loss 42087529.310484\n",
      "iteration 600 / 1500: loss 38713458.248443\n",
      "iteration 700 / 1500: loss 46430198.678090\n",
      "iteration 800 / 1500: loss 38722407.253171\n",
      "iteration 900 / 1500: loss 35448042.216505\n",
      "iteration 1000 / 1500: loss 38166352.487087\n",
      "iteration 1100 / 1500: loss 40583530.138067\n",
      "iteration 1200 / 1500: loss 46791929.891033\n",
      "iteration 1300 / 1500: loss 41620007.516992\n",
      "iteration 1400 / 1500: loss 42688466.879357\n",
      "iteration 0 / 1500: loss 4.916080\n",
      "iteration 100 / 1500: loss 29341821735622108909411756742262935522264250438307614116839642882711868788399396993412469278235804109735150448636854375032789102009790822304736925272759261956664927226985417141028296956926615232461241200029411479572709376.000000\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.475915\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.686245\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 21.283167\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 160.596896\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1524.394466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/classifiers/linear_classifier.py:70: RuntimeWarning: overflow encountered in multiply\n",
      "  self.W -= grad * learning_rate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15300.200959\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.371254\n",
      "iteration 100 / 1500: loss 240930607474149089607680.000000\n",
      "iteration 200 / 1500: loss 717407984541999351863048247604936704.000000\n",
      "iteration 300 / 1500: loss 2136190193849112596794966285785775072069338267648.000000\n",
      "iteration 400 / 1500: loss 6360827649844433565387009843401614454631675156866403485614080.000000\n",
      "iteration 500 / 1500: loss 18940321188405997330393372164554347155295444758637048825684267121218945024.000000\n",
      "iteration 600 / 1500: loss 56397655536029929821884188080529583907120065440303076056161221488587758956897305100288.000000\n",
      "iteration 700 / 1500: loss 167932503272842982499917642745651185565762334354225864730680979601113598955478761387431269844910080.000000\n",
      "iteration 800 / 1500: loss 500044290625996848547820738385557947461496461835421986832894825265610260019186157544534004133161503253527003136.000000\n",
      "iteration 900 / 1500: loss 1488957097134465675877105108037081658558713244662853966978223217114784116694432540259863239455438525137025513122193779720192.000000\n",
      "iteration 1000 / 1500: loss 4433593740929786701397497776682414985849724665945999462008871958401036281507285948549449440633136617775809201566272627189877352154791936.000000\n",
      "iteration 1100 / 1500: loss 13201692310303424796464528199997611007627085415029975654647526572309457419698438161336219358731661791081948051912536028964901489968030008938799824896.000000\n",
      "iteration 1200 / 1500: loss 39310024788011950933517623156121760688060658286505110766747886208813095788422488874917104051228381415404108669968177559504482930909684362015847568336488111276032.000000\n",
      "iteration 1300 / 1500: loss 117051512223783826850802751654350251856282507187573173256238571502411543816429113809162648600235083390648697347956749048730079827360124349932241158597892907655413986500804608.000000\n",
      "iteration 1400 / 1500: loss 348538485736414641290839396144125904109589771909626419235963752882081186173210662044493005783722608808746750659172224862603156835749445508158609339659808114991122333331576239469843447808.000000\n",
      "iteration 0 / 1500: loss 5.333017\n",
      "iteration 100 / 1500: loss 765889211576908780618126162463750608003641891230017801510685456406445060498919482430678048151398100107429297508663170694564190284157820865040258266439104096277228741018396769155678703905644324542280730891574499077185794712552318506712792497151783397315972537981717184512.000000\n",
      "iteration 200 / 1500: loss inf\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 4.802979\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.946825\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.068619\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.312509\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 21.657142\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 157.628910\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1551.558656\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15316.850355\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.166432\n",
      "iteration 100 / 1500: loss inf\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.181288\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.506489\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.017605\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.564492\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.402150\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 21.254840\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 157.520661\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1556.251052\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15214.992556\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.361933\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.272447\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.398698\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.589809\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.187222\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.675206\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 20.530273\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 157.219835\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1562.277389\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15359.637697\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.068868\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 6.257449\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 4.796168\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.657857\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 5.933586\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 7.968421\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 21.685280\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 159.724228\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 1518.999032\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "iteration 0 / 1500: loss 15261.665290\n",
      "iteration 100 / 1500: loss nan\n",
      "iteration 200 / 1500: loss nan\n",
      "iteration 300 / 1500: loss nan\n",
      "iteration 400 / 1500: loss nan\n",
      "iteration 500 / 1500: loss nan\n",
      "iteration 600 / 1500: loss nan\n",
      "iteration 700 / 1500: loss nan\n",
      "iteration 800 / 1500: loss nan\n",
      "iteration 900 / 1500: loss nan\n",
      "iteration 1000 / 1500: loss nan\n",
      "iteration 1100 / 1500: loss nan\n",
      "iteration 1200 / 1500: loss nan\n",
      "iteration 1300 / 1500: loss nan\n",
      "iteration 1400 / 1500: loss nan\n",
      "lr 1.000000e-10 reg 1.000000e-03 train accuracy: 0.110490 val accuracy: 0.116000\n",
      "lr 1.000000e-10 reg 1.000000e-02 train accuracy: 0.088286 val accuracy: 0.091000\n",
      "lr 1.000000e-10 reg 1.000000e-01 train accuracy: 0.086224 val accuracy: 0.089000\n",
      "lr 1.000000e-10 reg 1.000000e+00 train accuracy: 0.088327 val accuracy: 0.073000\n",
      "lr 1.000000e-10 reg 1.000000e+01 train accuracy: 0.081694 val accuracy: 0.086000\n",
      "lr 1.000000e-10 reg 1.000000e+02 train accuracy: 0.118367 val accuracy: 0.114000\n",
      "lr 1.000000e-10 reg 1.000000e+03 train accuracy: 0.127776 val accuracy: 0.143000\n",
      "lr 1.000000e-10 reg 1.000000e+04 train accuracy: 0.092388 val accuracy: 0.114000\n",
      "lr 1.000000e-10 reg 1.000000e+05 train accuracy: 0.083592 val accuracy: 0.091000\n",
      "lr 1.000000e-10 reg 1.000000e+06 train accuracy: 0.111327 val accuracy: 0.111000\n",
      "lr 1.668101e-08 reg 1.000000e-03 train accuracy: 0.164694 val accuracy: 0.177000\n",
      "lr 1.668101e-08 reg 1.000000e-02 train accuracy: 0.164857 val accuracy: 0.165000\n",
      "lr 1.668101e-08 reg 1.000000e-01 train accuracy: 0.177347 val accuracy: 0.176000\n",
      "lr 1.668101e-08 reg 1.000000e+00 train accuracy: 0.154857 val accuracy: 0.157000\n",
      "lr 1.668101e-08 reg 1.000000e+01 train accuracy: 0.156367 val accuracy: 0.161000\n",
      "lr 1.668101e-08 reg 1.000000e+02 train accuracy: 0.181776 val accuracy: 0.185000\n",
      "lr 1.668101e-08 reg 1.000000e+03 train accuracy: 0.179816 val accuracy: 0.181000\n",
      "lr 1.668101e-08 reg 1.000000e+04 train accuracy: 0.178102 val accuracy: 0.178000\n",
      "lr 1.668101e-08 reg 1.000000e+05 train accuracy: 0.283143 val accuracy: 0.303000\n",
      "lr 1.668101e-08 reg 1.000000e+06 train accuracy: 0.251367 val accuracy: 0.270000\n",
      "lr 2.782559e-06 reg 1.000000e-03 train accuracy: 0.383571 val accuracy: 0.365000\n",
      "lr 2.782559e-06 reg 1.000000e-02 train accuracy: 0.387612 val accuracy: 0.343000\n",
      "lr 2.782559e-06 reg 1.000000e-01 train accuracy: 0.386469 val accuracy: 0.351000\n",
      "lr 2.782559e-06 reg 1.000000e+00 train accuracy: 0.380653 val accuracy: 0.395000\n",
      "lr 2.782559e-06 reg 1.000000e+01 train accuracy: 0.381347 val accuracy: 0.358000\n",
      "lr 2.782559e-06 reg 1.000000e+02 train accuracy: 0.399633 val accuracy: 0.385000\n",
      "lr 2.782559e-06 reg 1.000000e+03 train accuracy: 0.396878 val accuracy: 0.397000\n",
      "lr 2.782559e-06 reg 1.000000e+04 train accuracy: 0.341265 val accuracy: 0.350000\n",
      "lr 2.782559e-06 reg 1.000000e+05 train accuracy: 0.250816 val accuracy: 0.254000\n",
      "lr 2.782559e-06 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e-03 train accuracy: 0.277816 val accuracy: 0.266000\n",
      "lr 4.641589e-04 reg 1.000000e-02 train accuracy: 0.297878 val accuracy: 0.285000\n",
      "lr 4.641589e-04 reg 1.000000e-01 train accuracy: 0.303714 val accuracy: 0.275000\n",
      "lr 4.641589e-04 reg 1.000000e+00 train accuracy: 0.304592 val accuracy: 0.272000\n",
      "lr 4.641589e-04 reg 1.000000e+01 train accuracy: 0.241224 val accuracy: 0.244000\n",
      "lr 4.641589e-04 reg 1.000000e+02 train accuracy: 0.171122 val accuracy: 0.164000\n",
      "lr 4.641589e-04 reg 1.000000e+03 train accuracy: 0.163878 val accuracy: 0.140000\n",
      "lr 4.641589e-04 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 4.641589e-04 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e-03 train accuracy: 0.303980 val accuracy: 0.324000\n",
      "lr 7.742637e-02 reg 1.000000e-02 train accuracy: 0.268857 val accuracy: 0.251000\n",
      "lr 7.742637e-02 reg 1.000000e-01 train accuracy: 0.264796 val accuracy: 0.259000\n",
      "lr 7.742637e-02 reg 1.000000e+00 train accuracy: 0.190429 val accuracy: 0.178000\n",
      "lr 7.742637e-02 reg 1.000000e+01 train accuracy: 0.133122 val accuracy: 0.137000\n",
      "lr 7.742637e-02 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 7.742637e-02 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e-03 train accuracy: 0.224245 val accuracy: 0.237000\n",
      "lr 1.291550e+01 reg 1.000000e-02 train accuracy: 0.173429 val accuracy: 0.202000\n",
      "lr 1.291550e+01 reg 1.000000e-01 train accuracy: 0.091551 val accuracy: 0.079000\n",
      "lr 1.291550e+01 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.291550e+01 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-03 train accuracy: 0.056327 val accuracy: 0.051000\n",
      "lr 2.154435e+03 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 2.154435e+03 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 3.593814e+05 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 5.994843e+07 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e-01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+00 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+01 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+02 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+03 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+04 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+05 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "lr 1.000000e+10 reg 1.000000e+06 train accuracy: 0.100265 val accuracy: 0.087000\n",
      "best validation accuracy achieved during cross-validation: 0.397000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = np.logspace(-10, 10, 10) # np.logspace(-10, 10, 8) #-10, -9, -8, -7, -6, -5, -4\n",
    "regularization_strengths = np.logspace(-3, 6, 10) # causes numeric issues: np.logspace(-5, 5, 8) #[-4, -3, -2, -1, 1, 2, 3, 4, 5, 6]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "for lr in learning_rates:\n",
    "    for rs in regularization_strengths:\n",
    "        cl_softmax = Softmax()\n",
    "        cl_softmax.train(X_train, y_train, learning_rate=lr, reg=rs,\n",
    "                              num_iters=1500, verbose=True)\n",
    "        y_train_pred = cl_softmax.predict(X_train)\n",
    "        training_accuracy = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = cl_softmax.predict(X_val)\n",
    "        validation_accuracy = np.mean(y_val == y_val_pred)\n",
    "        results[(lr, rs)] = (training_accuracy, validation_accuracy)\n",
    "        if best_val < validation_accuracy:\n",
    "            best_val = validation_accuracy\n",
    "            best_softmax = cl_softmax\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during cross-validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.374000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk8AAAF/CAYAAABQVS1eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1085ee650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
